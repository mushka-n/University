{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ace99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c391e822-aad8-4251-ab88-c95ba3660722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from tqdm import tqdm\n",
    "import gensim.downloader as api\n",
    "\n",
    "CPU = torch.device('cpu')\n",
    "GPU = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab4bef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    if isinstance(data, dict):\n",
    "        return dict((k, to_device(v, device)) for k, v in data.items())\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000c18c",
   "metadata": {},
   "source": [
    "# Предварительная обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30188a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './train_data.csv'\n",
    "\n",
    "class SalaryDataset(Dataset):\n",
    "    MIN_COUNT = 10\n",
    "    PAD, UNK = 'PAD', 'UNK'\n",
    "    PAD_IX, UNK_IX = 0, 1\n",
    "    TEXT_COLS = ['Title', 'FullDescription']\n",
    "    CATEGORIAL_COLS = ['Category', 'Company', 'LocationNormalized', 'ContractType', 'ContractTime']\n",
    "    TARGET_COL = 'Log1pSalary'\n",
    "    MAX_TITLE_LENGHT = 20\n",
    "    MAX_DESC_LENGHT = 500\n",
    "\n",
    "    def _process_data(self):\n",
    "        self._data[self.TARGET_COL] = np.log1p(self._data['SalaryNormalized']).astype('float32')\n",
    "        self._data[self.CATEGORIAL_COLS] = self._data[self.CATEGORIAL_COLS].fillna('NaN')\n",
    "        self._data[self.TEXT_COLS] = self._data[self.TEXT_COLS].fillna('NaN')\n",
    "        tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "        self._data[self.TEXT_COLS] = self._data[self.TEXT_COLS].applymap(lambda x: \" \".join(tokenizer.tokenize(x.lower())))\n",
    "        for col in self._data[self.TEXT_COLS]:\n",
    "            for line in self._data[col].values:\n",
    "                self._tok_cntr.update(line.split(\" \"))\n",
    "\n",
    "        self._tokens = sorted(t for t, c in self._tok_cntr.items() if c >= self.MIN_COUNT)\n",
    "        self._tokens = [self.PAD, self.UNK] + self._tokens\n",
    "        self._token_to_id = {t: i for i, t in enumerate(self._tokens)}\n",
    "        top_companies, top_counts = zip(*Counter(self._data['Company']).most_common(1000))\n",
    "        recognized_companies = set(top_companies)\n",
    "        self._data[\"Company\"] = self._data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "        self._categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False).fit(self._data[self.CATEGORIAL_COLS].apply(dict, axis=1))\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        self._data = pd.read_csv(path)\n",
    "        self._tok_cntr = Counter()\n",
    "        self._process_data()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self._data[i:i+1]\n",
    "\n",
    "        title = row['Title'].values[0]\n",
    "        desc = row['FullDescription'].values[0]\n",
    "\n",
    "        title_vals_encoded = [self._token_to_id.get(tok, self.UNK_IX) for tok in str.split(title, ' ')]\n",
    "        desc_vals_encoded = [self._token_to_id.get(tok, self.UNK_IX) for tok in str.split(desc, ' ')]\n",
    "\n",
    "        return {\n",
    "            'Title': title_vals_encoded,\n",
    "            'FullDescription': desc_vals_encoded,\n",
    "            self.TARGET_COL: row[self.TARGET_COL].values[0],\n",
    "            'Categorical': self._categorical_vectorizer.transform(row[self.CATEGORIAL_COLS].apply(dict, axis=1)).flatten().tolist()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba98da7",
   "metadata": {},
   "source": [
    "### Создаем датасет и разбиваем на тренировки / тесты / подмножества проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e512692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SalaryDataset(DATA_DIR)\n",
    "NUM_TOKENS = len(dataset._tokens)\n",
    "NUM_CAT_FEATURES = len(dataset._categorical_vectorizer.vocabulary_)\n",
    "\n",
    "train_size = round(len(dataset)*0.7)\n",
    "val_size = round((len(dataset) - train_size) * (1/3))\n",
    "test_size = (len(dataset) - train_size) - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], torch.Generator().manual_seed(12))\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    collated = dict(zip(data[0].keys(), [[], [], [], []]))\n",
    "\n",
    "    for d in data:\n",
    "        for k, v in d.items():\n",
    "            if k == 'Title':\n",
    "                v.extend([SalaryDataset.PAD_IX] * (SalaryDataset.MAX_TITLE_LENGHT - len(v))) # padding\n",
    "                v = v[:SalaryDataset.MAX_TITLE_LENGHT]\n",
    "                collated[k].append(v)\n",
    "            elif k == 'FullDescription':\n",
    "                v.extend([SalaryDataset.PAD_IX] * (SalaryDataset.MAX_DESC_LENGHT - len(v)))  # padding\n",
    "                v = v[:SalaryDataset.MAX_DESC_LENGHT]\n",
    "                collated[k].append(v)\n",
    "            else:\n",
    "                collated[k].append(v)\n",
    "\n",
    "    for k, v in collated.items():\n",
    "        t = torch.float32 if k in [SalaryDataset.TARGET_COL, 'Categorical'] else torch.int32\n",
    "        collated[k] = torch.as_tensor(v, dtype=t)\n",
    "        \n",
    "    return collated\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4ed24",
   "metadata": {},
   "source": [
    "# Глубокое обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9559e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalaryPredictor(nn.Module):\n",
    "    def __init__(self, n_tokens=NUM_TOKENS, n_cat_features=NUM_CAT_FEATURES, hid_size=8):\n",
    "        super().__init__()\n",
    "        self.n_tokens = n_tokens\n",
    "        self.n_cat_features = n_cat_features\n",
    "        self.hid_size = hid_size\n",
    "        self.embedder = nn.Embedding(n_tokens, hid_size)\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(hid_size, hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(hid_size, hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.categorical_encoder = nn.Sequential(\n",
    "            nn.Linear(n_cat_features, hid_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size * 2, hid_size * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(hid_size * 4, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        title_features = self.title_encoder(title_embeddings).squeeze()\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        description_features = self.description_encoder(description_embeddings).squeeze()\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "        features = torch.cat([title_features, description_features, categorical_features], dim=1)\n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c64ee",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59039406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device=None):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "\n",
    "    loader = val_loader if not device else DeviceDataLoader(val_loader, device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            pred = model(batch)\n",
    "            squared_error += torch.mean(torch.square(pred - batch[SalaryDataset.TARGET_COL]))\n",
    "            abs_error += torch.mean(torch.abs(pred - batch[SalaryDataset.TARGET_COL]))\n",
    "            num_samples += len(batch)\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93763d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02852ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoches=5, device=None, criterion=nn.MSELoss(reduction='mean')):\n",
    "    loader = train_loader\n",
    "\n",
    "    if device:\n",
    "        model.to(device)\n",
    "        loader = DeviceDataLoader(train_loader, device)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        model.train()\n",
    "        for batch in tqdm(loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch[SalaryDataset.TARGET_COL])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        mse, mae = evaluate(model, device)\n",
    "        print(f'Epoch: {epoch+1} | Loss: {loss.item()} | Validation: MSE={mse}/MAE={mae}')\n",
    "\n",
    "    if device:\n",
    "        model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa81ef0",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a8e7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device=None):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    loader = test_loader\n",
    "\n",
    "    if device:\n",
    "        model.to(device)\n",
    "        loader = DeviceDataLoader(test_loader, device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x in loader:\n",
    "            pred = model(x)\n",
    "            squared_error += torch.mean(torch.square(pred - x[SalaryDataset.TARGET_COL]))\n",
    "            abs_error += torch.mean(torch.abs(pred - x[SalaryDataset.TARGET_COL]))\n",
    "            num_samples += len(x)\n",
    "\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "\n",
    "    if device:\n",
    "        model.cpu()\n",
    "\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4024e",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85068f",
   "metadata": {},
   "source": [
    "### Стартовые показатели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6d49b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [04:20<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.2022046446800232 | Validation: MSE=1.1587030092875164/MAE=0.5278991063435873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [04:26<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.1151353195309639 | Validation: MSE=0.9814110596974691/MAE=0.48653586705525714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [04:30<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.1103941947221756 | Validation: MSE=0.9770181179046631/MAE=0.48610806465148926\n"
     ]
    }
   ],
   "source": [
    "model = SalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), epoches=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36b349",
   "metadata": {},
   "source": [
    "### Вместе с BatchNorm и LayerNorm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3aebd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLayerNormSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.LayerNorm(SalaryDataset.MAX_TITLE_LENGHT-1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.LayerNorm(SalaryDataset.MAX_DESC_LENGHT-1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.categorical_encoder = nn.Sequential(\n",
    "            nn.Linear(self.n_cat_features, self.hid_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hid_size * 2),\n",
    "            nn.Linear(self.hid_size * 2, self.hid_size * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(self.hid_size * 4, self.hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hid_size),\n",
    "            nn.Linear(self.hid_size, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011deeaa",
   "metadata": {},
   "source": [
    "### После интеграции BatchNorm и LayerNorm сеть стала справляться удачнее, лучше всего видно по значениям MSE/MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b623a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [04:40<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.2048853039741516 | Validation: MSE=0.05279832581679026/MAE=0.09047077099482219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [04:43<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.1174137145280838 | Validation: MSE=0.0376193051536878/MAE=0.07444887359937032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [05:01<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.10806610435247421 | Validation: MSE=0.03722138206164042/MAE=0.07187482217947642\n"
     ]
    }
   ],
   "source": [
    "model = BatchLayerNormSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), epoches=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6272bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParrallelConvSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title_encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=4),\n",
    "            nn.Dropout(p=0.33),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder2 = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=4),\n",
    "            nn.Dropout(p=0.33),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(self.hid_size * 6, self.hid_size * 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_size * 3, int(self.hid_size * 1.5)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(self.hid_size * 1.5), 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "            title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "            title_features = self.title_encoder(title_embeddings).squeeze()\n",
    "            title_features2 = self.title_encoder2(title_embeddings).squeeze()\n",
    "            description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "            description_features = self.description_encoder(description_embeddings).squeeze()\n",
    "            description_features2 = self.description_encoder2(description_embeddings).squeeze()\n",
    "            categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "            title_features = torch.cat((title_features, title_features2), dim=1)\n",
    "            description_features = torch.cat((description_features, description_features2), dim=1)\n",
    "            features = torch.cat([title_features, description_features, categorical_features], dim=1)\n",
    "            return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6c0af",
   "metadata": {},
   "source": [
    "### MSE и MAE улучшаются при добавлении параллельных сверточных слоев, однако совсем незначительно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecd3d300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [05:24<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.12488577514886856 | Validation: MSE=0.7543275356292725/MAE=0.42392945289611816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [05:09<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.0923488512635231 | Validation: MSE=0.6474840641021729/MAE=0.3924413522084554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [05:18<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.10246707499027252 | Validation: MSE=0.5841095447540283/MAE=0.3726075490315755\n"
     ]
    }
   ],
   "source": [
    "model = ParrallelConvSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b638acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedder = nn.Embedding(self.n_tokens, self.hid_size * 2)\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size * 2, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),                             \n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),     \n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.title_encoder2 = nn.Sequential(                           \n",
    "            nn.Conv1d(self.hid_size * 2, self.hid_size, kernel_size=4),\n",
    "            nn.Dropout(p=0.33),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size * 2, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),                             \n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),    \n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.description_encoder2 = nn.Sequential(                      \n",
    "            nn.Conv1d(self.hid_size * 2, self.hid_size, kernel_size=4),\n",
    "            nn.Dropout(p=0.33),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hid_size),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.categorical_encoder = nn.Sequential(                       \n",
    "            nn.Linear(self.n_cat_features, self.hid_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hid_size * 4),                           \n",
    "            nn.Linear(self.hid_size * 4, self.hid_size * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(                           \n",
    "            nn.Linear(self.hid_size * 6, self.hid_size * 3),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(self.hid_size * 3),                            \n",
    "            nn.Linear(self.hid_size * 3, int(self.hid_size * 1.5)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(int(self.hid_size * 1.5)),                   \n",
    "            nn.Linear(int(self.hid_size * 1.5), 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        title_features = self.title_encoder(title_embeddings).squeeze()\n",
    "        title_features2 = self.title_encoder2(title_embeddings).squeeze()\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        description_features = self.description_encoder(description_embeddings).squeeze()\n",
    "        description_features2 = self.description_encoder2(description_embeddings).squeeze()\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "        title_features = torch.cat((title_features, title_features2), dim=1)\n",
    "        description_features = torch.cat((description_features, description_features2), dim=1)\n",
    "        features = torch.cat((title_features, description_features, categorical_features), dim=1)\n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a10c40e",
   "metadata": {},
   "source": [
    "### Результаты стали еще лучше после того как я использовал и нормализацию и параллельные энкодеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db0720ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [05:53<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.10126442462205887 | Validation: MSE=0.029981888830661774/MAE=0.0659247636795044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1339/1339 [09:09<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.09590458869934082 | Validation: MSE=0.02421173204978307/MAE=0.05799533426761627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [08:57<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.08416073024272919 | Validation: MSE=0.02155221253633499/MAE=0.0541977733373642\n"
     ]
    }
   ],
   "source": [
    "model = NewSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e205f114",
   "metadata": {},
   "source": [
    "### Раняя остановка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b2dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(lst: list):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "def delta(x):\n",
    "    def delta_impl(y):\n",
    "        return x - y\n",
    "    return delta_impl\n",
    "\n",
    "def early_stop_train(model, optimizer, epoches=5, device=None, criterion=nn.MSELoss(reduction='mean')):\n",
    "    EXIT_CRITERION = 0.01\n",
    "    loader = train_loader\n",
    "    outputs: dict[str, list] = {}\n",
    "\n",
    "    if device:\n",
    "        model.to(device)\n",
    "        loader = DeviceDataLoader(train_loader, device)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        model.train()\n",
    "        for batch in tqdm(loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch[SalaryDataset.TARGET_COL])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        mse, mae = evaluate(model, device)\n",
    "        means = {k: avg(v[-5:]) for k,v in outputs.items()}\n",
    "        deltas = [means['Loss'] - loss.item(), means['mse'] - mse, means['mae'] - mae]\n",
    "\n",
    "        if any(x < EXIT_CRITERION for x in deltas):\n",
    "            return\n",
    "\n",
    "        outputs['Losses'].append(loss.item())\n",
    "        outputs['mse'].append(mse)\n",
    "        outputs['mae'].append(mae)\n",
    "    if device:\n",
    "        model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10006965",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff37ed",
   "metadata": {},
   "source": [
    "### Как работает pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf693b",
   "metadata": {},
   "source": [
    "Pooling слой это - фильтр заданного размера, он обрабатывает данную матрицу и возвращает число, которое зависит от алгоритма.\n",
    "\n",
    "Его основные цели: \n",
    " - Уменьшение изображения. Нужно для того чтобы последующие конволюции могли проводить операции над большей областью картинки.\n",
    " - Увеличение инвариантности выхода сети по отношению к малому переносу входа.\n",
    " - Оптимизация вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d3cfc",
   "metadata": {},
   "source": [
    "### Максимум по временной компоненте (незавимисо для каждой фичи)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e53ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(534, self.hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hid_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        max_pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        title_features = self.title_encoder(title_embeddings).squeeze().permute(0, 2, 1)\n",
    "        title_features = max_pool(title_features).squeeze() # Max pooling\n",
    "\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        description_features = self.description_encoder(description_embeddings).squeeze().permute(0, 2, 1)\n",
    "        description_features = max_pool(description_features).squeeze() # Max pooling\n",
    "\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "\n",
    "        features = torch.cat((title_features, description_features, categorical_features), dim=1)\n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766e641",
   "metadata": {},
   "source": [
    " Все что нам дал MaxPool - незначительно меньшее значение по loss  \n",
    " Остальные переменные показали лишь худший результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "605a6806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:36<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.1306263953447342 | Validation: MSE=0.8713125387827555/MAE=0.4564642111460368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:28<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.0991913452744484 | Validation: MSE=0.985065221786499/MAE=0.4879465103149414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:28<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.09645353257656097 | Validation: MSE=0.948194662729899/MAE=0.4796563386917114\n"
     ]
    }
   ],
   "source": [
    "model = MaxPoolSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d60c2e",
   "metadata": {},
   "source": [
    "Среднее по временной компоненте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c6f5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPoolSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.title_avgPooler = nn.AvgPool1d(19, stride=19, count_include_pad=False)\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(self.hid_size, self.hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.desc_avgPooler = nn.AvgPool1d(499, stride=499, count_include_pad=False)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        title_features = self.title_encoder(title_embeddings).squeeze()\n",
    "        title_features = self.title_avgPooler(title_features).squeeze()\n",
    "\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        description_features = self.description_encoder(description_embeddings).squeeze()\n",
    "        description_features = self.desc_avgPooler(description_features).squeeze()\n",
    "\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "        features = torch.cat((title_features, description_features, categorical_features), dim=1)\n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4e791",
   "metadata": {},
   "source": [
    "C AveragePooling результаты заметно лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a69b4a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:26<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.1536521464586258 | Validation: MSE=0.042825246850649513/MAE=0.0805329829454422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:54<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.14230093359947205 | Validation: MSE=0.03107058008511861/MAE=0.06736072897911072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:29<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.11169379204511642 | Validation: MSE=0.026431108514467876/MAE=0.061901321013768516\n"
     ]
    }
   ],
   "source": [
    "model = AvgPoolSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f96c62",
   "metadata": {},
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aac66e",
   "metadata": {},
   "source": [
    "Предобученный эмбеддинг (весы заморожены)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e54fd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedFrozenSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self, wordvec: torch.FloatTensor, hid_size=100):\n",
    "        super().__init__(hid_size=100)\n",
    "        self.embedder = nn.Embedding.from_pretrained(wordvec, freeze=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "775f604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c94da",
   "metadata": {},
   "source": [
    "Ошибка все еще высокая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14f61688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [06:18<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.09619984775781631 | Validation: MSE=1.437311013539632/MAE=0.594262440999349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [06:19<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.08519542962312698 | Validation: MSE=1.4581607182820637/MAE=0.5993243058522543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [06:27<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.07579339295625687 | Validation: MSE=1.46799898147583/MAE=0.6016062498092651\n"
     ]
    }
   ],
   "source": [
    "model = PretrainedFrozenSalaryPredictor(torch.FloatTensor(kv.vectors))\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26756fdd",
   "metadata": {},
   "source": [
    "Предобученный эмбеддинг (весы обучаются)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b8899b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedEmbedSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self, wordvec: torch.FloatTensor, hid_size=100):\n",
    "        super().__init__(hid_size=hid_size)\n",
    "        self.embedder = nn.Embedding.from_pretrained(wordvec, freeze=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57408c04",
   "metadata": {},
   "source": [
    "Тут ситуация та же"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c4b7d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [12:31<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.09913275390863419 | Validation: MSE=1.3804882367451985/MAE=0.5826433499654134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [12:35<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.08199077844619751 | Validation: MSE=1.323238452275594/MAE=0.5709047714869181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [12:40<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.07381653040647507 | Validation: MSE=1.379287560780843/MAE=0.5832666953404745\n"
     ]
    }
   ],
   "source": [
    "model = PretrainedEmbedSalaryPredictor(torch.FloatTensor(kv.vectors))\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a3b3b",
   "metadata": {},
   "source": [
    "# Задание 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4aeba",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "368d6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.title_lstm = nn.LSTM(20, self.hid_size, bidirectional=bidirectional)\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.desc_lstm = nn.LSTM(500, self.hid_size, bidirectional=bidirectional)\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        lstm_out, (hn, cn) = self.title_lstm(title_embeddings)\n",
    "        title_features = self.title_encoder(lstm_out).squeeze()\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        lstm_out, (hn, cn) = self.desc_lstm(description_embeddings)\n",
    "        description_features = self.description_encoder(lstm_out).squeeze()\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "        features = torch.cat(\n",
    "            [title_features, description_features, categorical_features], dim=1)\n",
    "        \n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28346f28",
   "metadata": {},
   "source": [
    "LSTM && (bidirectional = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8d536a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:51<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.158545583486557 | Validation: MSE=0.1733523408571879/MAE=0.18522685766220093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:52<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.15152324736118317 | Validation: MSE=0.16154774030049643/MAE=0.1777780850728353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:53<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.14832158386707306 | Validation: MSE=0.12742688258488974/MAE=0.15458577871322632\n"
     ]
    }
   ],
   "source": [
    "model = LSTMSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb20cf",
   "metadata": {},
   "source": [
    "LSTM && (bidirectional = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84c45f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [05:47<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.13777832686901093 | Validation: MSE=0.4909350474675496/MAE=0.3366178274154663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [05:54<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.14693059027194977 | Validation: MSE=0.5267475843429565/MAE=0.3501235644022624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [05:57<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.11374281346797943 | Validation: MSE=0.4604838689168294/MAE=0.3277652859687805\n"
     ]
    }
   ],
   "source": [
    "model = LSTMSalaryPredictor(bidirectional=True)\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72444b00",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a7d283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUSalaryPredictor(SalaryPredictor):\n",
    "    def __init__(self, bidirectional=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title_gru = nn.GRU(20, self.hid_size, bidirectional=bidirectional)\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.desc_gru = nn.GRU(500, self.hid_size, bidirectional=bidirectional)\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0, 2, 1)\n",
    "        gru_out, hn = self.title_gru(title_embeddings)\n",
    "        title_features = self.title_encoder(gru_out).squeeze()\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0, 2, 1)\n",
    "        gru_out, hn = self.desc_gru(description_embeddings)\n",
    "        description_features = self.description_encoder(gru_out).squeeze()\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "        features = torch.cat([title_features, description_features, categorical_features], dim=1)\n",
    "        return self.final_predictor(features).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da9198",
   "metadata": {},
   "source": [
    "GRU && (bidirectional = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e7ba1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:59<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.148161843419075 | Validation: MSE=0.23544867833455405/MAE=0.22281877199808756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [04:58<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.14018207788467407 | Validation: MSE=0.24832107623418173/MAE=0.23035502433776855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [05:53<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.14000648260116577 | Validation: MSE=0.2331114411354065/MAE=0.22225689888000488\n"
     ]
    }
   ],
   "source": [
    "model = GRUSalaryPredictor()\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece779c",
   "metadata": {},
   "source": [
    "GRU && (bidirectional = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0551313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [06:39<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.14410187304019928 | Validation: MSE=0.3469020128250122/MAE=0.2782339851061503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [06:36<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.13265176117420197 | Validation: MSE=0.3661278486251831/MAE=0.2873084743817647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1339/1339 [06:37<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 0.12114362418651581 | Validation: MSE=0.36182117462158203/MAE=0.287251075108846\n"
     ]
    }
   ],
   "source": [
    "model = GRUSalaryPredictor(bidirectional=True)\n",
    "train(model, torch.optim.Adam(model.parameters(), lr=1e-3), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971a61b",
   "metadata": {},
   "source": [
    "LSTM сработал лучше"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
